# -*- coding: utf-8 -*-
"""Slope_Stats_FINAL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZYqqJv9NK6prbKwmWvoEGvBUs-8K5zPg

Version: 1.1
Author: Kenneth Cassar
Date: 10-07-2022

The following Python script was adapted from Vollrath, Mullissa and Reiche 2020: "Angular-based radiometric slope correction for Sentinel-1 on Google Earth Engine". The GitHub repository for the paper is: https://github.com/ESA-PhiLab/radiometric-slope-correction

This script implements the slope correction on the roi and calculates various statistics used in the evaluation of the best slope correction model according to the land cover type.

The script is implemented in Google Colab and statistiscal data are saved in the user's Google Drive
"""

import ee
# Trigger the authentication flow.
ee.Authenticate()

import numpy as np
ee.Initialize()

"""1. Defining the slope correction module"""

def slope_correction(collection, elevation, model, buffer=0):
    '''This function applies the slope correction on a collection of Sentinel-1 data
       
       :param collection: ee.Collection of Sentinel-1
       :param elevation: ee.Image of DEM
       :param model: model to be applied (volume/surface)
       :param buffer: buffer in meters for layover/shadow amsk
        
        :returns: ee.Image
    '''
    
    def _volumetric_model_SCF(theta_iRad, alpha_rRad):
        '''Code for calculation of volumetric model SCF
        
        :param theta_iRad: ee.Image of incidence angle in radians
        :param alpha_rRad: ee.Image of slope steepness in range
        
        :returns: ee.Image
        '''
        
        # create a 90 degree image in radians
        ninetyRad = ee.Image.constant(90).multiply(np.pi/180)
        
        # model
        nominator = (ninetyRad.subtract(theta_iRad).add(alpha_rRad)).tan()
        denominator = (ninetyRad.subtract(theta_iRad)).tan()
        return nominator.divide(denominator) 
    
    
    def _surface_model_SCF(theta_iRad, alpha_rRad, alpha_azRad):
        '''Code for calculation of direct model SCF
        
        :param theta_iRad: ee.Image of incidence angle in radians
        :param alpha_rRad: ee.Image of slope steepness in range
        :param alpha_azRad: ee.Image of slope steepness in azimuth
        
        :returns: ee.Image
        '''
        
        # create a 90 degree image in radians
        ninetyRad = ee.Image.constant(90).multiply(np.pi/180)
        
        # model  
        nominator = (ninetyRad.subtract(theta_iRad)).cos()
        denominator = (alpha_azRad.cos()
          .multiply((ninetyRad.subtract(theta_iRad).add(alpha_rRad)).cos()))

        return nominator.divide(denominator)


    def _erode(image, distance):
      '''Buffer function for raster

      :param image: ee.Image that shoudl be buffered
      :param distance: distance of buffer in meters
        
      :returns: ee.Image
      '''
      
      d = (image.Not().unmask(1)
          .fastDistanceTransform(30).sqrt()
          .multiply(ee.Image.pixelArea().sqrt()))
    
      return image.updateMask(d.gt(distance))
    
    
    def _masking(alpha_rRad, theta_iRad, buffer):
        '''Masking of layover and shadow
        
        
        :param alpha_rRad: ee.Image of slope steepness in range
        :param theta_iRad: ee.Image of incidence angle in radians
        :param buffer: buffer in meters
        
        :returns: ee.Image
        '''
        # layover, where slope > radar viewing angle 
        layover = alpha_rRad.lt(theta_iRad).rename('layover')

        # shadow 
        ninetyRad = ee.Image.constant(90).multiply(np.pi/180)
        shadow = alpha_rRad.gt(ee.Image.constant(-1).multiply(ninetyRad.subtract(theta_iRad))).rename('shadow')
        
        # add buffer to layover and shadow
        if buffer > 0:
            layover = _erode(layover, buffer)   
            shadow = _erode(shadow, buffer)  

        # combine layover and shadow
        no_data_mask = layover.And(shadow).rename('no_data_mask')
        
        return layover.addBands(shadow).addBands(no_data_mask)
                        
        
    def _correct(image):
        '''This function applies the slope correction and adds layover and shadow masks
        
        '''
        
        # get the image geometry and projection
        geom = image.geometry()
        proj = image.select(1).projection()
        
        # calculate the look direction
        heading = (ee.Terrain.aspect(image.select('angle'))
                                     .reduceRegion(ee.Reducer.mean(), geom, 1000)
                                     .get('aspect'))
                   

        # Sigma0 to Power of input image
        sigma0Pow = ee.Image.constant(10).pow(image.divide(10.0))

        # the numbering follows the article chapters
        # 2.1.1 Radar geometry 
        theta_iRad = image.select('angle').multiply(np.pi/180)
        phi_iRad = ee.Image.constant(heading).multiply(np.pi/180)
        
        # 2.1.2 Terrain geometry
        alpha_sRad = ee.Terrain.slope(elevation).select('slope').multiply(np.pi/180).setDefaultProjection(proj).clip(geom)
        phi_sRad = ee.Terrain.aspect(elevation).select('aspect').multiply(np.pi/180).setDefaultProjection(proj).clip(geom)
        
        # we get the height, for export 
        height = elevation.setDefaultProjection(proj).clip(geom)
        
        # 2.1.3 Model geometry
        #reduce to 3 angle
        phi_rRad = phi_iRad.subtract(phi_sRad)

        # slope steepness in range (eq. 2)
        alpha_rRad = (alpha_sRad.tan().multiply(phi_rRad.cos())).atan()

        # slope steepness in azimuth (eq 3)
        alpha_azRad = (alpha_sRad.tan().multiply(phi_rRad.sin())).atan()

        # local incidence angle (eq. 4)
        theta_liaRad = (alpha_azRad.cos().multiply((theta_iRad.subtract(alpha_rRad)).cos())).acos()
        theta_liaDeg = theta_liaRad.multiply(180/np.pi)

        # 2.2 
        # Gamma_nought
        gamma0 = sigma0Pow.divide(theta_iRad.cos())
        gamma0dB = ee.Image.constant(10).multiply(gamma0.log10()).select(['VV', 'VH'], ['VV_gamma0', 'VH_gamma0'])
        ratio_gamma = (gamma0dB.select('VV_gamma0')
                        .subtract(gamma0dB.select('VH_gamma0'))
                        .rename('ratio_gamma0'))

        if model == 'volume':
            scf = _volumetric_model_SCF(theta_iRad, alpha_rRad)

        if model == 'surface':
            scf = _surface_model_SCF(theta_iRad, alpha_rRad, alpha_azRad)

        # apply model for Gamm0_f
        gamma0_flat = gamma0.divide(scf)
        gamma0_flatDB = (ee.Image.constant(10)
                         .multiply(gamma0_flat.log10())
                         .select(['VV', 'VH'],['VV_gamma0flat', 'VH_gamma0flat'])
                        )

        masks = _masking(alpha_rRad, theta_iRad, buffer)

        # calculate the ratio for RGB vis
        ratio_flat = (gamma0_flatDB.select('VV_gamma0flat')
                        .subtract(gamma0_flatDB.select('VH_gamma0flat'))
                        .rename('ratio_gamma0flat')
                     )

        return (image.rename(['VV_sigma0', 'VH_sigma0', 'incAngle'])
                      .addBands(gamma0dB)
                      .addBands(ratio_gamma)
                      .addBands(gamma0_flatDB)
                      .addBands(ratio_flat)
                      .addBands(alpha_rRad.rename('alpha_rRad'))
                      .addBands(alpha_azRad.rename('alpha_azRad'))
                      .addBands(phi_sRad.rename('aspect'))
                      .addBands(alpha_sRad.rename('slope'))
                      .addBands(theta_iRad.rename('theta_iRad'))
                      .addBands(theta_liaRad.rename('theta_liaRad'))
                      .addBands(masks)
                      .addBands(height.rename('elevation'))
                 )    
    
    # run and return correction
    return collection.map(_correct)

"""2. 
*   Defining the AOI
*   Loading the UKECH 10m land cover file from Google Earth Engine (GEE) assets
*   Filtering the Sentinel-1 imagery
*   Loading the BlueSky 5m DEM from Google Earth Engine (GEE) assets
*   Looping (Mapping) through the slope correction module calculations

NOTE: The computations are done on the Google Server and take several hours to complete. At the end the computations are stored within respective folders created in the user's Google Drive 




"""

# geometry for AOI in Hollin Hill North Yorkshire
geometry = ee.Geometry.Polygon([[[-1.231403738843886, 54.291069000328001 ], [ -0.687709680649565, 54.289328250942106 ], [ -0.679296058617742, 53.914486883179535 ], [ -1.228212364969746, 53.913906633384236 ], [ -1.231403738843886, 54.291069000328001]]])

# Corine land cover (2018) of Hollin HIll
lc = ee.Image('users/kennethcassar/LC/LCM_10m').rename('landcover')

# filter Sentinel-1 collection for AOI and selected dates
s1Collection = ee.ImageCollection('COPERNICUS/S1_GRD') \
        .filterBounds(geometry) \
        .filterDate('2019-01-01', '2021-12-31')


# path to dem 
dem = ee.Image('users/kennethcassar/DEM/Bluesky_DTM_Clip')

# list of models
models = ['volume', 'surface']

# this is the scale we want the data to be sampled
scale = 10 

# loop through all combinations and export to drive
for model in models:
    
    # get the respective collection and bands and mosaic to a single image
    corrected_image = slope_correction(
        s1Collection, 
        ee.Image(dem), 
        model
    ).mosaic()

    # we get geometry and projection of the image
    proj = corrected_image.select(1).projection()
    geom = corrected_image.clip(geometry).select(1).geometry()

    # add lc and bring everything to same projection/geometry
    added_LC = corrected_image.addBands(lc)
    image_reprojected = added_LC.reproject(proj, scale=scale).clip(geom)
      
    # get the bandlist 
    bandlist = image_reprojected.bandNames().getInfo()
    
    # create an export job for each band
    for band in bandlist:
        task = ee.batch.Export.image.toDrive(
            image=image_reprojected.select(band).clip(geom),
            description=band,
            folder='slope_correction_LCM/{}_{}_buf_0'.format(dem, model),
            fileNamePrefix=band,
            region=geom.coordinates().getInfo(),
            scale=scale,
            maxPixels=1e12
            )
        task.start()
        print(task.status())

"""3. Downloading of necessary Python libraries"""

!apt-get -qq install -y libspatialindex-dev
!pip install -q --upgrade earthpy

"""4. Access the Google Drive path to eventually save the results in """

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
import os
import itertools

import numpy as np
import pandas as pd 
from scipy import stats
from scipy import optimize

import rasterio as rio
from rasterio.windows import Window

import matplotlib.pyplot as plt
import seaborn as sns
import earthpy.plot as ep
# %matplotlib inline

5. Read the necessary bands calculated under step 2

list_of_layers = [
    'VV_gamma0', 'VH_gamma0',
    'VV_gamma0flat', 'VH_gamma0flat',
    'alpha_rRad', 'theta_liaRad', 'aspect', 
    'layover', 'shadow',     
    'landcover']

# paths to dem and model types
dem = 'Bluesky_DTM_Clip'
models = ['volume', 'surface']
buffers = [0]

modelDict = {}

# loop thorugh all combinations and put into dictionary
for model, buffer in itertools.product(models, buffers):

    key = '{}_{}_buf_{}'.format(dem, model, buffer)
    # here we read all layers into a dictionary
    dataDict = {}
    for layer in list_of_layers:
        with rio.open('/content/drive/My Drive/slope_correction_LCM {}/{}.tif'.format(key, layer)) as src:
            print('Reading ' + layer)
            dataDict[layer] = np.nan_to_num(src.read(window=Window(0, 340, 3000, 3000)))[0]
            print(dataDict[layer].shape)
    
    # write respective dataDict to our model dict, where different models are stored
    modelDict[key] = dataDict

"""6. Calculate the statistics and plot the results as charts. At this stage the charts are just stored as temporary files and not visualised."""

def create_plot_aspect_against_backscatter(model, data, array, mask, stats_dict, outfile, gridsize):
    
    # getlayer info #
    polarisation = layer.split('_')[0]
    flat = layer.split('_')[1][-4:]
    if flat != 'flat':
        flat = False
    
    # fore and backslope lines
    look_angle = 33.089022518635176 # hardcoded 
    backslope = look_angle
    foreslope = backslope + 180
    vertical_y = np.linspace(-28, 8, 5)
    fs_x = 0 * vertical_y + foreslope
    bs_x = 0 * vertical_y + backslope
    
    # calculate mean line
    horizontal_x = np.linspace(0, 360, 10) 
    mean_y = 0 * horizontal_x + stats_dict['mean']
    
    if not flat:
        y_label = r'$\gamma^0$ [dB]'
    else:
        y_label = r'$\gamma^0_f$ [dB]'
    
    # check for 0s in aspect and update mask
    data['aspect'][data['aspect'] == 0] = np.nan
    mask = mask & np.isfinite(data['aspect'])
    aspect_deg_masked = np.subtract(to_deg(data['aspect'][mask]), 180)
    aspect_deg_masked = to_deg(data['aspect'][mask])

    # plot
    # surpress plotting, since we only want to save the files
    plt.ioff()
    X = sns.jointplot(aspect_deg_masked, array[mask], kind='hex', gridsize=(gridsize, gridsize))
    X.ax_joint.plot(horizontal_x, mean_y, 'k--', linewidth=.75)
    X.ax_joint.plot(fs_x, vertical_y, 'k--', linewidth=.75)
    X.ax_joint.plot(bs_x, vertical_y, 'k--', linewidth=.75)
    X.ax_joint.set_xlabel(r'Aspect angle $\phi_s$ [deg]', fontsize=14)
    X.ax_joint.set_ylabel(y_label,  fontsize=14)
    X.ax_joint.set_ylim(-30, 10)
    #X.ax_joint.set_xlim(-190, 185)
    X.ax_joint.set_xlim(-10, 365)
    
    # add textbox with ampl, mean and sd
    props = dict(boxstyle='round', facecolor='lightgrey', alpha=0.5)
    textstr = '\n'.join((
        r'$\mathrm{A}=%.2f$' % (stats_dict['amplitude'], ),
        r'$\mu=%.2f$' % (stats_dict['mean'], ),
        r'$\sigma=%.2f$' % (stats_dict['sd'], )))
    X.ax_joint.text(290, -28,textstr, fontsize=10, bbox=props)
    
    # add title
    if not flat:
        title = '{} Backscatter modulation by slopes'.format(polarisation)
    else:
        title = '{} Backscatter after Model {}'.format(polarisation, model)
        
    plt.suptitle(title, x=0.45, y=1.02, fontweight='bold', fontsize=14)
    
    # save
    plt.savefig(outfile, bbox_inches='tight', pad_inches=0.5)
    plt.close()

def create_plot_alpha_against_backscatter(model, data, array, mask, stats_dict, outfile, gridsize):
    
    # getlayer info #
    polarisation = layer.split('_')[0]
    flat = layer.split('_')[1][-4:]
    if flat != 'flat':
        flat = False
    
    # calculate mean line
    horizontal_x = np.linspace(-40, 40, 10) 
    mean_y = 0 * horizontal_x + stats_dict['mean']
    
    if not flat:
        y_label = r'$\gamma^0$ [dB]'
    else:
        y_label = r'$\gamma^0_f$ [dB]'
    
    # check for 0s in aspect and update mask
    data['alpha_rRad'][data['alpha_rRad'] == 0] = np.nan
    mask = mask & np.isfinite(data['alpha_rRad'])
    alpha_deg_masked = to_deg(data['alpha_rRad'][mask])

    # plot
    # surpress plotting, since we only want to save the files
    plt.ioff()
    X = sns.jointplot(alpha_deg_masked, array[mask], kind='hex', gridsize=(gridsize, gridsize))
    X.ax_joint.plot(horizontal_x, mean_y, 'k--', linewidth=.75)
    X.ax_joint.set_xlabel(r'Slope Steepness in range $\alpha$ [deg]', fontsize=14)
    X.ax_joint.set_ylabel(y_label, fontsize=14)
    X.ax_joint.set_ylim(-30, 10)
    X.ax_joint.set_xlim(-45, 45)
    
    # add textbox with ampl, mean and sd
    props = dict(boxstyle='round', facecolor='lightgrey', alpha=0.5)
    textstr = '\n'.join((
        r'$\mathrm{s}=%.2f$' % (stats_dict['slope'], ),
        r'$\mu=%.2f$' % (stats_dict['mean'], ),
        r'$\sigma=%.2f$' % (stats_dict['sd'], )))
    X.ax_joint.text(20, -28,textstr, fontsize=10, bbox=props)
    
    # add title
    if not flat:
        title = '{} Backscatter modulation by slopes'.format(polarisation)
    else:
        title = '{} Backscatter after Model {}'.format(polarisation, model)
        
    plt.suptitle(title, x=0.45, y=1.02, fontweight='bold', fontsize=14)
    
    # save
    plt.savefig(outfile, bbox_inches='tight', pad_inches=0.5)
    plt.close()

def create_plot_lia_against_backscatter(model, data, array, mask, stats_dict, outfile, gridsize):
    
    
    # getlayer info #
    polarisation = layer.split('_')[0]
    flat = layer.split('_')[1][-4:]
    if flat != 'flat':
        flat = False
    
    # calculate mean line
    horizontal_x = np.linspace(0, 90, 10) 
    mean_y = 0 * horizontal_x + stats_dict['mean']
    
    if not flat:
        y_label = r'$\gamma^0$ [dB]'
    else:
        y_label = r'$\gamma^0_f$ [dB]'
    
    # check for 0s in aspect and update mask
    data['theta_liaRad'][data['theta_liaRad'] == 0] = np.nan
    mask = mask & np.isfinite(data['theta_liaRad'])
    theta_deg_masked = to_deg(data['theta_liaRad'][mask])

    # plot
    # surpress plotting, since we only want to save the files
    plt.ioff()
    X = sns.jointplot(theta_deg_masked, array[mask], kind='hex', gridsize=(gridsize, gridsize))
    X.ax_joint.plot(horizontal_x, mean_y, 'k--', linewidth=.75)
    X.ax_joint.set_xlabel(r'Local Incidence Angle $\theta$ [deg]', fontsize=14)
    X.ax_joint.set_ylabel(y_label, fontsize=14)
    X.ax_joint.set_ylim(-30, 10)
    X.ax_joint.set_xlim(-10, 100)
    
    # add textbox with ampl, mean and sd
    props = dict(boxstyle='round', facecolor='lightgrey', alpha=0.5)
    textstr = '\n'.join((
        #r'$\mathrm{s}=%.2f$' % (stats_dict['slope'], ),
        r'$\mu=%.2f$' % (stats_dict['mean'], ),
        r'$\sigma=%.2f$' % (stats_dict['sd'], )))
    X.ax_joint.text(-5, -28,textstr, fontsize=10, bbox=props)
    
    # add title
    if not flat:
        title = '{} Backscatter modulation by slopes'.format(polarisation)
    else:
        title = '{} Backscatter after Model {}'.format(polarisation, model)
        
    plt.suptitle(title, x=0.45, y=1.02, fontweight='bold', fontsize=14)
    
    # save
    plt.savefig(outfile, bbox_inches='tight', pad_inches=0.5)
    plt.close()

# from rad to degree
def to_deg(rad):
    return np.multiply(rad,  np.divide(180,np.pi))

# sin function or curve fitting
def sin_func(x, a, b, c):
    return a * np.sin(x + b) - c

def tf_stats(data, array, layer, lc_class, mask):
    
    #------------------------------------------------
    # slope calculation
    
    # mask alpha angle
    data['alpha_rRad'][data['alpha_rRad'] == 0] = np.nan
    mask_alpha = mask & np.isfinite(data['alpha_rRad'])
    alpha_deg_masked = np.subtract(to_deg(data['alpha_rRad'][mask_alpha]), 180)
    
    # mask out nans
    x = array.copy()
    x[~mask_alpha] = np.nan
    x = x[np.logical_not(np.isnan(x))] 
    y = alpha_deg_masked
    y = y[np.logical_not(np.isnan(y))]
    
    # lin-regression
    slope, intercept, r_value, p_value, std_err = stats.linregress(y, x)        
    #------------------------------------------------
    
    #------------------------------------------------
    # amplitude calculation
    
    # mask aspect 0s and nans
    data['aspect'][data['aspect'] == 0] = np.nan
    mask_aspect = mask & np.isfinite(data['aspect'])
    aspect_deg_masked = np.subtract(data['aspect'][mask_aspect], np.pi)
    
    # mask out nans
    x = array.copy()
    x[~mask_aspect] = np.nan
    x = x[np.logical_not(np.isnan(x))]
    y = aspect_deg_masked
    y = y[np.logical_not(np.isnan(y))]
   
    # curve fitting
    params, params_covariance = optimize.curve_fit(sin_func, y, x)
    amp = params[0]
    #------------------------------------------------

    # mean, sd
    mean = np.nanmean(array[mask_alpha])
    std = np.nanstd(array[mask_alpha])
    
    # create final dictionary
    stat_dict = {'lc_class': lc_class, 
                 'layer': layer, 
                 'count': np.sum(mask), 
                 'mean': mean, 
                 'sd': std, 
                 'slope': slope, 
                 'amplitude': np.abs(amp)
                }
    
    return stat_dict

"""7. Define the land cover classes according to the loaded land cover file under step 2. The calculated statistics are then mapped against the land cover classes."""

# list of class names
legend_entries = ["Broad-leaved forest", "Arable & Horticulture", "Improved Grassland", "Calcareous Grassland", "Heather Grassland"]
# list of class values
ras_values = [1, 3, 4, 6, 10]

# prepare columns for new dataframe
df_cols = ['lc_class', 'layer', 'count', 'mean', 'sd', 'slope', 'amplitude']

# paths to dem and model types and buffer
dem = 'Bluesky_DTM_Clip'
models = ['surface', 'volume']
buffers = [0] 

# loop through different models and buffers
for model, buffer in itertools.product(models, buffers):
    
    # get respective arrays within the model/datadict
    key = '{}_{}_buf_{}'.format(dem, model, buffer)
    dataDict = modelDict[key]
    
    print(' INFO: Creating figures and stats for {} {} with buffer {}.'
      .format(dem, model, buffer)
    )
    # create empty dataframe for statistics
    df_stats = pd.DataFrame(columns=df_cols)
    
    # crate outdirectory where plots and stats will be saved
    outdir = '/content/drive/My Drive/slope_correction_LCM/pictures/{}/'.format(key)
    os.makedirs(outdir, exist_ok=True)
    
    # loop through classes and respective raster values file
    for legend_entry, ras_value in zip(legend_entries, ras_values):
        
        # set raster value respective to class
        print(' INFO: Analysing {} with raster value {}.'
          .format(legend_entry, ras_value)
        )

        # loop thorugh different corrected and uncorrected layers
        for layer in ['VV_gamma0', 'VV_gamma0flat', 'VH_gamma0', 'VH_gamma0flat']:
                        
            # create combined Land Cover and Layover/Shadow mask
            valid_data_mask = (
                [dataDict['landcover'] == ras_value] & 
                (dataDict['layover'] > 0) & 
                (dataDict['shadow'] > 0)
            )[0] 

            # apply this mask and add valid data mask of backscatter array 
            array = dataDict[layer].copy()
            array[array == 0] = np.nan
            mask = valid_data_mask & np.isfinite(array)

            # set everything else to nan
            array[~mask] = np.nan

            # for some classes array might be empty, so we add an if
            if True in np.unique(np.isfinite(array)):
                
                # calculate stats
                stats_dict = tf_stats(
                    dataDict.copy(), array, layer, legend_entry, mask
                )
                stats_dict['lc_class_code'] =  ras_value

                # and put into pandas dataframe
                df = pd.DataFrame([stats_dict], columns=stats_dict.keys())
                df_stats = df_stats.append(stats_dict, ignore_index=True)

                # plotting
                gridsize=100
                model_nr = '1' if model == 'volume' else '2'

                create_plot_aspect_against_backscatter(
                    model_nr, dataDict.copy(), array, mask, stats_dict, 
                    '{}/{}_{}_vs_aspect.png'.format(outdir, legend_entry, layer), 
                    gridsize
                )

                create_plot_alpha_against_backscatter(
                    model_nr, dataDict.copy(), array, mask, stats_dict, 
                    '{}/{}_{}_vs_slope.png'.format(outdir, legend_entry, layer), 
                    gridsize
                )

                create_plot_lia_against_backscatter(
                    model_nr, dataDict.copy(), array, mask, stats_dict, 
                    '{}/{}_{}_vs_LIA.png'.format(outdir, legend_entry, layer), 
                    gridsize
                )

        # save the complete stas dataframe to pickle
        df_stats.reset_index()
        df_stats.to_pickle('{}/stats.pickle'.format(outdir))

"""8. Save and visualise the statistical data computed for each land cover class within the AOI. """

df_stats_dict = {}

# paths to dem and model types
dem = 'Bluesky_DTM_Clip'
models = ['surface', 'volume'] 

# this is for the concatenation of VV and VH stats
concat_cols = ['lc_class', 'VV', 'Pixel count', 'VV mean', 'VV SD', 'VV slope', 'VV amp', 
                'VH', 'VH mean', 'VH SD', 'VH slope', 'VH amp']

# create empty list for adding all stats within the for loop
df_con_merged = []

# loop through different dems and models
for model in models:

    # get respective arrays within the datadict
    key = '{}_{}_buf_0'.format(dem, model)
    
    # read each df into the dictionary
    outdir = '/content/drive/My Drive/slope_correction_LCM/pictures/{}/'.format(key)
    df_stats = pd.read_pickle('{}/stats.pickle'.format(outdir))
    
    # split into vv and vh
    df_vv = df_stats[df_stats['layer'].str.contains('VV')].reset_index().rename(columns={'layer': 'VV-pol'})
    df_vh = df_stats[df_stats['layer'].str.contains('VH')].reset_index().rename(columns={'layer': 'VH-pol'})
    
    # concat vv and vh columns
    df_con = pd.concat([df_vv, df_vh], axis=1, ignore_index=True)
    
    # rename columns
    df_con.columns = ['i_2', 'lc_class', 'VV', 'Pixel count', 'VV mean', 'VV SD', 'VV slope', 'VV amp', 'lc class code',
                      'i_3', 'lc_class_2', 'VH', 'VH count', 'VH mean', 'VH SD', 'VH slope', 'VH amp', 'lc class code_2']
    
    # subset columns
    df_con = df_con[['lc_class', 'lc class code', 'VV', 'Pixel count', 'VV mean', 'VV SD', 'VV slope', 'VV amp', 
                     'VH', 'VH mean', 'VH SD', 'VH slope', 'VH amp']]
    
     # rename the layer names 
    df_con['VV'] = df_con['VV'].str.replace('VV_gamma0flat', '{}'.format(key))
    df_con['VH'] = df_con['VH'].str.replace('VH_gamma0flat', '{}'.format(key))
    
    df_con['VV'] = df_con['VV'].str.replace('VV_gamma0', 'Original')
    df_con['VH'] = df_con['VH'].str.replace('VH_gamma0', 'Original')
    
    # merge to existent dfs
    df_con_merged.append(df_con)


# bring all the data together
appended_data = pd.concat(df_con_merged)

# exclude marginal classes
appended_data = appended_data[appended_data['Pixel count'] >= 100000]
appended_data = appended_data[['lc_class', 'VV', 'VV mean', 'VV SD', 'VV slope', 'VV amp', 'VH mean', 'VH SD', 'VH slope', 'VH amp']]
    

# rename model names
appended_data['VV'] = appended_data['VV'].str.replace('Bluesky_DTM_Clip_volume_buf_0', 'Model I')
appended_data['VV'] = appended_data['VV'].str.replace('Bluesky_DTM_Clip_surface_buf_0', 'Model II')

# remove double entries for original data and reindex 
appended_data.drop_duplicates(subset=['lc_class', 'VV'], keep='first', inplace=True)
appended_data = appended_data.set_index(['lc_class', 'VV']).sort_values(['lc_class', 'VV'], ascending=False)

# rename model column
appended_data